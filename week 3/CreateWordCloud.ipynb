{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Create a word cloud\n",
    "\n",
    "In this notebook we give a high level summary of a large English text.\n",
    "\n",
    "We do that in the form of a wordcloud.\n",
    "\n",
    "### Here is the text in PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe src=http://maartenmarx.nl/teaching/CollectieveIntelligentie/Data/thesis-main_final.pdf width=700 height=700></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: pdftotext: command not found\r\n"
     ]
    }
   ],
   "source": [
    "# turn the pdf into a text file \n",
    "\n",
    "# Files staan op http://maartenmarx.nl/teaching/CollectieveIntelligentie/Data/\n",
    "\n",
    "!pdftotext -layout  -enc UTF-8 ../Data/thesis-main_final.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Hoe groot is de file en hoeveel woorden staan erin\n",
    "!ls -lh ../Data/thesis-main_final.*; wc -w ../Data/thesis-main_final.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Hoe ziet de tekst er uit?\n",
    "!head -30 ../Data/thesis-main_final.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "from pattern.en import  lemma  # also exists for Dutch\n",
    "import pattern.nl #  import lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# After a lot of hassle we discovered how to fix the encoding problem....\n",
    "\n",
    "text= open('../Data/thesis-main_final.txt').read().decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "TextTokens= nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "len(TextTokens),TextTokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text normalization\n",
    "\n",
    "1. remove interpunction tokens\n",
    "2. lowercase\n",
    "3. lemmatize\n",
    "4. remove short words\n",
    "5. remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect the stopwords\n",
    "stopwords= set(nltk.corpus.stopwords.words('english')) # we turn it into a set because of speed\n",
    "len(stopwords), list(stopwords)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "TextTokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Text normalization Try out\n",
    "\n",
    "[lemma(s.lower()) for s in TextTokens[:20] \n",
    "     if not  re.match(r'^\\W+$',s) and \n",
    "        len(s) >3 and\n",
    "         not s in stopwords\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Do it for all \n",
    "NormalizedTextTokens= [lemma(s.lower()) for s in TextTokens\n",
    "     if not  re.match(r'^\\W+$',s) and \n",
    "        len(s) >3 and\n",
    "         not s in stopwords\n",
    "]\n",
    "\n",
    "# Do some repairs \n",
    "# try re.sub(u'\\ufb01','fi', u'de\\ufb01nition')\n",
    "# with lemmatization we get new 3 letter words\n",
    "NormalizedTextTokens=[re.sub(u'\\ufb01','fi',w) for w in NormalizedTextTokens if len(w) >3 ]\n",
    "\n",
    "# See the reduction in number of tokens and number of types\n",
    "for x in [NormalizedTextTokens,TextTokens]:\n",
    "    print len(x), len(set(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now count\n",
    "\n",
    "WordCloud= Counter(NormalizedTextTokens)\n",
    "WordCloud.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Change to Wordle format: see http://www.wordle.net/advanced\n",
    "# remove/repair \"wrong\" words\n",
    "\n",
    "for p in WordCloud.most_common(51):\n",
    "    if not p[0]=='defin':\n",
    "        print \"%s:%s\" % (p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tadah: Make a wordcloud\n",
    "\n",
    "I pasted the above into <http://www.wordle.net/advanced> and got \n",
    "\n",
    "<img src='../Data/thesis-main_final.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Nu een NL voorbeeld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!wget http://stichtinggezondheid.nl/wp-content/uploads/2013/09/Van-Partners-naar-Ouders-PDF-eBook-Oei-ik-groei.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pdftotext -layout  -enc UTF-8 Van-Partners-naar-Ouders-PDF-eBook-Oei-ik-groei.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect the stopwords\n",
    "stopwords= set(nltk.corpus.stopwords.words('dutch')) # we turn it into a set because of speed\n",
    "len(stopwords), list(stopwords)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "text= open('Van-Partners-naar-Ouders-PDF-eBook-Oei-ik-groei.txt').read().decode('utf-8')\n",
    "TextTokens= nltk.word_tokenize(text)\n",
    "# Do it for all \n",
    "NormalizedTextTokens= [ (s.lower()) for s in TextTokens\n",
    "     if not  re.match(r'^\\W+$',s) and \n",
    "        len(s) >3 \n",
    "         and         not s in stopwords\n",
    "]\n",
    "\n",
    "# Do some repairs \n",
    "# try re.sub(u'\\ufb01','fi', u'de\\ufb01nition')\n",
    "# with lemmatization we get new 3 letter words\n",
    "NormalizedTextTokens=[re.sub(u'\\ufb01','fi',w) for w in NormalizedTextTokens if len(w) >3 ]\n",
    "\n",
    "# See the reduction in number of tokens and number of types\n",
    "for x in [NormalizedTextTokens,TextTokens]:\n",
    "    print len(x), len(set(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "WordCloud= Counter(NormalizedTextTokens)\n",
    "WordCloud.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Verbeteringen\n",
    "\n",
    "* lemmatiseer\n",
    "* pak alleen de zelfstandige naamwoorden, de werkwoorden in de lijst hierboven zegen niet veel, en er zitten toch ook nog veel \"stopwoorden\" tussen die je dan ook wel kwijt bent.\n",
    "* Beide kan met de `pattern.nl` module.\n",
    "\n",
    "### Zelf doen\n",
    "* Neem een wat langere Nederlandse tekst, bijvoorbeeld een (liefst dik) boek dat je goed kent, en doe hetzelfde. \n",
    "* Wat vind je, geeft je woordenwolk een mooi beeld van dat boek?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Science Project\n",
    "\n",
    "* Verbeter de woorden in de wolken:\n",
    "    * gooi \"domein-specifieke\" stopwoorden eruit: _chapter, section, page,..._\n",
    "    * Voeg bigrams/ colocaties toe _\"computer science\", \"set theory\", \"brain damage\", etc_\n",
    "* Integreer de code met een download en run code over een heel corpus van een hoogleraar.\n",
    "    * Download alle proefschriften van PhDs van die hoogleraar via uva dare\n",
    "    * draai de software erover heen\n",
    "    * Maak een uniforme verzameling wolken: allicht geordend in de tijd\n",
    "    * Maak ook een samenvatting van alle proefschriften bij elkaar.\n",
    "    * Maak ook een tijdsdiagram: waarbij je \"trends\" in die proefschriften van die hoogleraar aangeeft.\n",
    "\n",
    "* Doe dit voor \n",
    "    * Dick Swaab\n",
    "    * JFAK van Benthem\n",
    "    * Maarten de Rijke\n",
    "    * Iemand uit een totaal ander vakgebied \n",
    "\n",
    "\n",
    "* Hieronder een voorbeeld met Dick Swaab.\n",
    "    * Werkt perfect: je zoekt op een promotor, en download alles wat daaronder zit.\n",
    "    * in `Swaab/dare.uva.nl/document/2` zitten alle pdfs.\n",
    "        * 21 proefschriften, 271 files, 532 Mb in totaal.\n",
    "    * Elk proefschrift lijkt er dubbel in te zitten omdat het er zowel als geheel als in de afzonderlijke hoofstukken in zit. \n",
    "    * Jammer genoeg zitten de proefschriften niet in aparte folders.\n",
    "    * Maar via de search-pages zijn de files wel weer aan de metadata te hangen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mkdir Swaab\n",
    "%cd Swaab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -r recursief. -np \"no-parents\" (ga alleen naar beneden met ophalen, niet naar boven). -q geef geen output\n",
    "!wget -r -np -q  \"http://dare.uva.nl/search?field1=promotoren+copromotoren&value1=swaab&join=and&field2=authors+editors&value2=+&smode=advanced&publicatiejaar-min=&publicatiejaar-max=&documenttype=&documenttype-join=or\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
