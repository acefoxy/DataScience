{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment2\n",
    "\n",
    "## Notebook made by  \n",
    "\n",
    "|** Name** | **Student id** | **email**|\n",
    "|:- |:-|:-|\n",
    "|. | | |\n",
    "|  | |. |\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. \n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img src='link to your selfie'/>\n",
    "\n",
    "### Note\n",
    "* **Assignments without the selfies or completely filled in information will not be graded and receive 0 points.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: obtaining information from the web\n",
    "\n",
    "### RSS parsing\n",
    "\n",
    "Make a notebook that performs the following steps.\n",
    "\n",
    "1. Create a script that retrieves all urls of rss feeds from <http://www.volkskrant.nl/rss/feeds/>. Use urllib2 and beautifulsoup for this. Store the urls in a list.\n",
    "    * **update 2016**\n",
    "    * As all Dutch sites, Volkskrant asks whether you accept cookies. This makes simple collecting webpages a lot harder. \n",
    "    * The code in the code cell below does the trick. \n",
    "    * After running this, I could collect further files from Volkskrant without additional cookie hassle.\n",
    "2. Download all rss feeds and store them on disk.\n",
    "3. Parse all RSS feeds using `lxml`. Create a list of  dicts with fields \"channel\", \"url\", \"title\", \"date\" in which you store this information for each item.\n",
    "4. Compute some statistics about this dict: how many items, how many per channel, are there doubles (items occuring in several channels), etc.\n",
    "5. Write this list as a csv file, store on disk, and upload to Google fusion tables.\n",
    "6. Download all articles (once), parse out the text and store as pairs (date,text) in a list.\n",
    "7. Count per day the number of words, and the number of unique words. Show this in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cookielib # Thanks to http://stackoverflow.com/questions/29395407/enabling-cookies-with-urllib\n",
    "import urllib2\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "\n",
    "\n",
    "url =   'http://www.volkskrant.nl/rss/feeds/'\n",
    "\n",
    "# with urllib2 and handling cookies\n",
    "cookiejar= cookielib.LWPCookieJar()\n",
    "opener= urllib2.build_opener( urllib2.HTTPCookieProcessor(cookiejar) )\n",
    "response=opener.open(url)\n",
    "html_doc= ' '.join(response.readlines())\n",
    " \n",
    "rsssoup = BeautifulSoup(html_doc, \"lxml\")\n",
    "\n",
    "\n",
    "# test \n",
    "list_items=[ref['href'] for ref in  rsssoup.findAll('a')  if ref.get('href')]\n",
    "len(list_items), list_items[91:124]\n",
    "list_items = list_items[91:124]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#download all files and store them on disk\n",
    "for i in range(len(list_items)):\n",
    "    url = list_items[i]\n",
    "    filename = url[25:29]+'.xml'\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    with open(filename, \"wb\") as code:\n",
    "        code.write(r.content)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parse all RSS feeds using lxml. Create a list of dicts with fields \"channel\", \"url\", \"title\", \"date\" in which you store this information for each item.\n",
    "from lxml import etree\n",
    "from lxml import objectify\n",
    "\n",
    "#need channels\n",
    "#parse with single files in lists  \n",
    "path = '/Users/macbook/Desktop/DS/week1/hi/'\n",
    "files = !ls '/Users/macbook/Desktop/DS/week1/hi/'\n",
    "for filee in files:\n",
    "    with open(path+filee, 'r') as f:\n",
    "        parsed = etree.parse(f)\n",
    "        titles=parsed.xpath('//channel//title')\n",
    "        titlestrings= [ t.text for t in titles]\n",
    "        links=parsed.xpath('//channel//link')\n",
    "        linkstrings= [ t.text for t in links]\n",
    "        dates=parsed.xpath('//channel//pubDate')\n",
    "        datestrings = [ t.text for t in dates]\n",
    "        channel=parsed.xpath('//channel//title')\n",
    "        channelstrings = [ t.text for t in channel]\n",
    "        channels = [i for i in channelstrings if i.startswith('VK:')]\n",
    "        title_dict = [{'channel':x, 'title':y, 'link':z, 'date':a} for x in channels for y in titlestrings for z in linkstrings for a in datestrings]\n",
    "\n",
    "        \n",
    "titles_dict = [{'channel':x, 'title':y, 'link':z, 'date':a} for x in channels for y in titlestrings for z in linkstrings for a in datestrings]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "myfile = open('text.csv', 'wb')\n",
    "wr = csv.writer(myfile)\n",
    "wr.writerow(titles_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from lxml import objectify\n",
    "import requests\n",
    "\n",
    "path = '/Users/macbook/Desktop/DS/week1/hi/'\n",
    "files = !ls '/Users/macbook/Desktop/DS/week1/hi/'\n",
    "for filee in files:\n",
    "    with open(path+filee, 'r') as f:\n",
    "        parsed = etree.parse(f)\n",
    "        links=parsed.xpath('//channel//link')\n",
    "        linkstrings= [ t.text for t in links]\n",
    "        \n",
    "        for i in range(len(linkstrings)):\n",
    "            url = linkstrings[i][:200]\n",
    "            r = requests.get(url)\n",
    "            data = r.content\n",
    "            print data{:100}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### JSON parsing\n",
    "\n",
    "1. Download <http://maartenmarx.nl/teaching/DataScience/NoteBooks/consuming-json-data-from-a-web-service.ipynb>, remove all code blocks, and turn it into a notebook again. \n",
    "2. Check that what you did is correct and you did not remove too much using a notebook viewer.\n",
    "3. Now extract all code from the downloaded notebook, save it to a file, and execute it as a Python script. Does it give errors? Is it syntactically correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#See https://docs.python.org/2/howto/urllib2.html\n",
    "import json\n",
    "import urllib2\n",
    "url = \"http://maartenmarx.nl/teaching/DataScience/NoteBooks/consuming-json-data-from-a-web-service.ipynb\"\n",
    "jsonfile= urllib2.urlopen(url)\n",
    "\n",
    "json_as_python_object = json.load(jsonfile) # The josnfile transformed into a Python dict\n",
    "json_as_python_object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_as_python_object\n",
    "\n",
    "with open('data.py', 'w') as outfile:\n",
    "    json.dump(json_as_python_object, outfile)\n",
    "\n",
    "# it does give errors. It is horrible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PDF parsing\n",
    "\n",
    "1. Save a wordfile as PDF, open it in Python, extract all text. Describe the differences, if any. Try the same with a two column PDF file from the web. This exercise gets more interesting if you use _difficult_ PDF. Why not try <http://wch.github.io/latexsheet/latexsheet.pdf>?\n",
    "\n",
    "* Is the word order still as it should be?\n",
    "* What about the strange characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/macbook/Desktop/DS/week1\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/bin/pdf2txt.py\", line 116, in <module>\n",
      "    if __name__ == '__main__': sys.exit(main(sys.argv))\n",
      "  File \"//anaconda/bin/pdf2txt.py\", line 104, in main\n",
      "    fp = file(fname, 'rb')\n",
      "IOError: [Errno 2] No such file or directory: 'latexsheet.pdf'\n"
     ]
    }
   ],
   "source": [
    "!pwd /Users/macbook/Desktop/pdfminer\n",
    "\n",
    "#the latex file is pretty good. strange caracters are good in latex but the order is horrible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 2  Monty Hall\n",
    "\n",
    "# Q1 (Monty Hall problem)\n",
    "\n",
    "This is the part you hand in from the [warm up notebook](MontyHall.ipynb).\n",
    "\n",
    "##  This counts for your mark\n",
    "\n",
    "Generalize your simulation code to handle the case of `n` doors.\n",
    "\n",
    "\n",
    "This sentence can be read in two ways, giving very different outcomes! You must program both, and understand that it all makes sense.\n",
    "\n",
    "\n",
    "### Your work\n",
    "For both interpretations, give a plot with the number of doors on the x-axis and the win_percentage on the y-axis, and plot that value for the two strategies for numbers of doors between 3 and 20. \n",
    "\n",
    "Describe briefly what is going on here, and why the plots makes sense.\n",
    "\n",
    "\n",
    "### Interpretation  1\n",
    "* There are _n_ doors.\n",
    "* You have chosen one door.\n",
    "* The host opens one door as before (it is not the door you have chosen and not a door with the prize)\n",
    "* Now you switch or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Interpretation 2\n",
    "* There are _n_ doors.\n",
    "* You have chosen one door.\n",
    "* The host opens _all except two doors_, namely your door, and a random other door.  Behind none of the opened doors lies the prize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Uitleg \n",
    "\n",
    "* hier komt jullie uitleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
